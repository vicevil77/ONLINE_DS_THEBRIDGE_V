{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Configurar la GPU antes de inicializar TensorFlow\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSL FUNCIONES GPU CON LIBRERIAS SIMILARES PARA DATASCIENTIST\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "import dask_cudf\n",
    "from cuml.decomposition import PCA\n",
    "import glob\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask.dataframe as dd\n",
    "import dask_cudf as dd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import funciones_ML as bt\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, LabelEncoder, Normalizer, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay,classification_report,r2_score,RocCurveDisplay,confusion_matrix, accuracy_score,recall_score,f1_score,precision_score,precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, SGDClassifier, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "\"\"\"from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from sklearn.cluster import KMeans\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\"\"\"\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "# Configurar pandas para mostrar todas las filas y columnas\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Código para los tamaños de las fuentes y los \"ticks\" de los ejes:\n",
    "plt.rc('font', size=6)\n",
    "plt.rc('axes', labelsize=10, titlesize=10)\n",
    "plt.rc('legend', fontsize=10)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_ml\u001b[38;5;241m=\u001b[39m\u001b[43mcudf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.02/lib/python3.9/site-packages/nvtx/nvtx.py:116\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 116\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.02/lib/python3.9/site-packages/cudf/io/csv.py:88\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, prefix, mangle_dupe_cols, dtype, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, dayfirst, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, comment, delim_whitespace, byte_range, use_python_file_object, storage_options, bytes_per_thread)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_scalar(na_values):\n\u001b[1;32m     86\u001b[0m     na_values \u001b[38;5;241m=\u001b[39m [na_values]\n\u001b[0;32m---> 88\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mlibcudf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelim_whitespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelim_whitespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipinitialspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipinitialspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbyte_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_blank_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_blank_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# There exists some dtypes in the result columns that is inferred.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Find them and map them to the default dtypes.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     specified_dtypes \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n",
      "File \u001b[0;32mcsv.pyx:426\u001b[0m, in \u001b[0;36mcudf._lib.csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "df_ml=cudf.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo CSV gigante\n",
    "file_path = \"/mnt/d/Cursos/REPOSITORIOS/DATASET/malware_total/original/df_one_hot_full_ml_mm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "df_list = []\n",
    "# Leer los archivos CSV en paralelo\n",
    "for file in glob.glob(file_path):\n",
    "    df_list.append(dd.from_cudf(cudf.read_csv(file), npartitions=4))\n",
    "\n",
    "# dividir DataFrame origen  en 4\n",
    "full_df = [df.to_delayed() for df in df_list]\n",
    "\n",
    "# Verificar la forma del DataFrame completo\n",
    "print(full_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25011003 entries, 0 to 25011002\n",
      "Columns: 288 entries, Unnamed: 0 to history_nan\n",
      "dtypes: float64(287), int64(1)\n",
      "memory usage: 53.7 GB\n"
     ]
    }
   ],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_cudf.core.DataFrame'>\n",
      "Columns: 288 entries, Unnamed: 0 to history_nan\n",
      "dtypes: float64(287), int64(1)"
     ]
    }
   ],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo CSV gigante\n",
    "file_path = \"/mnt/d/Cursos/REPOSITORIOS/DATASET/malware_total/original/df_one_hot_full_ml_mm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dividir_dataset_en_lotes(file_path, tamano_lote):\n",
    "    \"\"\"\n",
    "    Divide un dataset en lotes utilizando cuDF.\n",
    "    \n",
    "    Args:\n",
    "    - file_path: Ruta al archivo CSV.\n",
    "    - tamano_lote: Tamaño de cada lote.\n",
    "    \n",
    "    Returns:\n",
    "    - Una lista de DataFrames cuDF, donde cada DataFrame representa un lote del dataset.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV completo en un DataFrame cuDF\n",
    "    df_completo = cudf.read_csv(file_path)\n",
    "    \n",
    "    # Calcular el número total de filas en el DataFrame\n",
    "    num_filas = len(df_completo)\n",
    "    \n",
    "    # Calcular el número total de lotes\n",
    "    num_lotes = (num_filas + tamano_lote - 1) // tamano_lote\n",
    "    \n",
    "    # Dividir el DataFrame en lotes\n",
    "    lotes = []\n",
    "    for i in range(num_lotes):\n",
    "        inicio = i * tamano_lote\n",
    "        fin = min((i + 1) * tamano_lote, num_filas)\n",
    "        lote = df_completo.iloc[inicio:fin]\n",
    "        lotes.append(lote)\n",
    "    \n",
    "    return lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = file_path  # Ruta al archivo CSV\n",
    "tamano_lote = 2500000  # Tamaño de cada lote\n",
    "\n",
    "# Dividir el dataset en lotes\n",
    "df_lotes = dividir_dataset_en_lotes(file_path, tamano_lote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el prefijo para los nombres de los archivos de salida\n",
    "output_file_prefix = \"lote_\"\n",
    "\n",
    "# Iterar sobre los dataframes y guardar cada uno en un archivo CSV\n",
    "for i, df_lote in enumerate(df_lotes):\n",
    "    output_file_path = f\"{output_file_prefix}{i}.csv\"  # Nombre del archivo de salida\n",
    "    df_lote.to_csv(output_file_path, index=False)  # Guardar el dataframe en el archivo CSV\n",
    "\n",
    "print(\"¡Guardado exitoso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA INVREMENTAL\n",
    "# Carga el dataset en un array de NumPy\n",
    "data = np.load(\"ruta/al/dataset.npy\")\n",
    "\n",
    "# Divide el dataset en batches de 100.000 filas\n",
    "batches = np.array_split(data, 250)\n",
    "\n",
    "# Crea un objeto PCA incremental con 10 componentes principales\n",
    "pca = IncrementalPCA(n_components=10, whiten=True, copy=True)\n",
    "\n",
    "for batch in batches:\n",
    "    # Actualiza el objeto PCA con el batch actual\n",
    "    pca.partial_fit(batch)\n",
    "\n",
    "# Obtén las componentes principales\n",
    "components = pca.components_\n",
    "\n",
    "# Obtén las varianzas explicadas por cada componente principal\n",
    "variances = pca.explained_variance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las componentesprincipales en un archivo\n",
    "np.save(\"ruta/a/las/componentes.npy\", components)\n",
    "\n",
    "# Guarda las varianzas explicadas en un archivo\n",
    "np.save(\"ruta/a/las/varianzas.npy\", variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Supongamos que tienes una lista de 12 DataFrames llamada df_list\n",
    "\n",
    "# Convertir cada DataFrame a un tensor y almacenarlos en una lista\n",
    "tensor_list = [tf.constant(df.values) for df in df_list]\n",
    "\n",
    "# Concatenar los tensores en uno solo\n",
    "full_tensor = tf.concat(tensor_list, axis=0)\n",
    "\n",
    "# Supongamos que quieres dividir el tensor en 3 partes\n",
    "num_splits = 3\n",
    "split_tensors = tf.split(full_tensor, num_splits)\n",
    "\n",
    "# Ahora split_tensors contendrá una lista de tensores divididos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  número de componentes principales\n",
    "n_components = 50  \n",
    "# Inicializar el modelo de PCA\n",
    "pca = cuml.PCA(n_components=n_components)\n",
    "\n",
    "# Iterar sobre los chunks del conjunto de datos\n",
    "for chunk in df_ml.to_delayed():\n",
    "    # Leer el chunk actual\n",
    "    data_chunk = chunk.compute()\n",
    "    \n",
    "    # Realizar PCA en el chunk actual\n",
    "    pca.partial_fit(data_chunk)\n",
    "\n",
    "# Combinar los resultados de PCA de cada chunk\n",
    "transformed_data = None\n",
    "for chunk in df_ml.to_delayed():\n",
    "    data_chunk = chunk.compute()\n",
    "    transformed_chunk = pca.transform(data_chunk)\n",
    "    if transformed_data is None:\n",
    "        transformed_data = transformed_chunk\n",
    "    else:\n",
    "        transformed_data = dask_cudf.concat([transformed_data, transformed_chunk])\n",
    "\n",
    "# Convierte el resultado en un DataFrame para su posterior análisis\n",
    "df_transformed = transformed_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PROBAMOS UN RANDONFOREST DE ML CON CONJUNTO DE DATOS TF-DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cudf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supongamos que tienes 4 DataFrames llamados df1, df2, df3 y df4\n",
    "\n",
    "# Definir el nombre de la columna objetivo\n",
    "target_column = \"target_column_name\"  # Reemplaza \"target_column_name\" por el nombre real de tu columna objetivo\n",
    "\n",
    "# Definir listas para almacenar las características (features) y objetivos (targets) de cada DataFrame\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "# Iterar sobre cada DataFrame para realizar el split\n",
    "for df in [df1, df2, df3, df4]:\n",
    "    # Dividir el DataFrame en características y objetivo\n",
    "    X, y = split_features_and_target(df, target_column)\n",
    "    # Agregar las características y objetivos a las listas\n",
    "    all_X.append(X)\n",
    "    all_y.append(y)\n",
    "\n",
    "# Concatenar todas las características y objetivos\n",
    "X = cudf.concat(all_X)\n",
    "y = cudf.concat(all_y)\n",
    "\n",
    "# Ahora tienes un único conjunto de características (X) y un único conjunto de objetivos (y) que puedes usar para modelar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<MapDataset element_spec=({'': TensorSpec(shape=(128,), dtype=tf.int32, name=None), '0': TensorSpec(shape=(128,), dtype=tf.float32, name=None), '1': TensorSpec(shape=(128,), dtype=tf.float32, name=None), '2': TensorSpec(shape=(128,), dtype=tf.float32, name=None), '3': TensorSpec(shape=(128,), dtype=tf.float32, name=None), '4': TensorSpec(shape=(128,), dtype=tf.float32, name=None), '5': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'proto_icmp': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'proto_tcp': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'proto_udp': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_OTH': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_REJ': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_RSTO': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_RSTOS0': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_RSTR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_RSTRH': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_S0': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_S1': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_S2': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_S3': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_SF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_SH': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'conn_state_SHR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Aa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Ar': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_C': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_CCC': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_CCCC': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_D': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DAd': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DFafA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DT': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DTT': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_D^': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_D^d': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DaFfA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Dd': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DdA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DdAa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DdAaFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DdAtaFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Dr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_DrF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_F': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Fa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_FaAr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_FaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_FfA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Ffa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Fr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaADdFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaDdAFTf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaDdAFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaDdAfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaDdAr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaDdR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaDdTAFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaFfA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_HafFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_I': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_R': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_S': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SAD': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SI': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SahAdDF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SahAdDFRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SahAdDFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SahAdDrfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_SahAdDtFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAD': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADCaGcgd': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADCaGdfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFadfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFadfRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFar': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFfa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaCGcgdF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaCGdt': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaCGdtfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaCGr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADacdtfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADad': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadfrr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadftFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadtFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadtRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadtcfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadtctfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadtctfFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadtfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadttFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadttcfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADadttfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADafdtF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADafr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADar': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADarfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdFaf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADda': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdaf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdafR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdattFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdfFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdtaFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADdtatFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfFrr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfaF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfdtFaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfdtR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfrF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADfrFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShADrfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFafR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFdRfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFdfRt': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAadDFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAadDFRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAadDFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAadDfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAadDr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAar': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAaw': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAaww': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdD': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDFaf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDFar': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDTafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRRRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRRfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRfRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFRr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFT': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFTf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFfRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaFrR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaRRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaRr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaT': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTFRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTFfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTRft': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTRr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTTRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTfRr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaTfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafFrR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafFrr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafrFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDafrR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaft': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaftF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDaftFR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDar': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDarfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDarr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatFrR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatRRR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDatrfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDfFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDfFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDtaFf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDtaFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDtaR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDtafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdDtafFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdFaRf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdFaf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdaDR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdaFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdfDF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdfDFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdfDr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdfFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdtDaFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdtDaFrR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdtDafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAdtfFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfdtDFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfdtDr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfdtF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfdtFa': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAfr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShAr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShArR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShArr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShDadAf': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShrA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShwA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_ShwAr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_Sr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^aA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^aR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^c': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^d': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^dDA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^dtt': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^hA': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^hADFr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^hADadfR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^hADafF': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^hADr': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^hR': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_^r': TensorSpec(shape=(128,), dtype=tf.float32, name=None), 'history_nan': TensorSpec(shape=(128,), dtype=tf.float32, name=None)}, TensorSpec(shape=(128,), dtype=tf.float32, name=None))>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia=model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importancia = pd.DataFrame({\"importancia\":importancia,\"Columnas\":columnas})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(df_importancia['Columnas'], df_importancia['importancia'])\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Importancia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMUESTREO DE LA CLASE MALICIOUS QYE ES LA MAYORITARIA PARA CONSEGUIR UN BALANCEO.- SE BALANCEA EL DATASET COMPLETO, DEJANDO LA TRAGET 50/50, PERO PERDIENDO UN 30% DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.Target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica submuestro, ya que la clase minoritaria la ingle yo con datos de la misma columna\n",
    "X = df_ml.drop('Target', axis=1)  \n",
    "y = df_ml['Target']    \n",
    "\n",
    "sampling_strategy=\"majority\"#solo submuestre la clase mayoritaria\n",
    "\n",
    "ros = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una columna temporal\n",
    "df_temp = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "# Re-indexa el dataframe\n",
    "df_temp = df_temp.reset_index(drop=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_temp.Target, bins=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_sca.iloc[:,0:12]\n",
    "y=df_sca.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MODELO MACHINE LEARENING LOGISTICREGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo logisctic con C para regule el sobreajutes y la funcion sag q es adecuado con muchos datos\n",
    "model = LogisticRegression(C=10, solver=\"sag\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTILIZANDO TECNICAS DE MACHINE LEARNING TANTO, DESBALANCEADO O BALANCEADA LA TARGET, CON RANDONFOREST Y LOGISTICREGRESION, LOS RESULTADO SOSN MUY BUENOS, PERO PUDIERA HABER UN SOBREAJUSTE, AUNQUE AL LOGISTICREGRESSION SE LE APLICADO UN VALOR MUY AL TO DE C PARA CONTENER EL SOBREAJUSTE, QUIZAS SEA UN MODELO PERFECTO, PERO VAMOS A PROBAR CON TECNICAS DE DEEP LEARNING.<br>\n",
    "\n",
    "CONTINUAMOS EN NOTEBOOK MALWARE_1_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
