{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MUTUAL INFORMATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento matemático para obtener la Información Mutua entre dos variables categóricas implica calcular cuánta información se comparte entre estas variables, es decir, cuánto reduce el conocimiento de una variable la incertidumbre acerca de la otra. La Información Mutua se define formalmente en términos de la entropía, que es una medida de la incertidumbre o la dispersión en la distribución de probabilidad de una variable.\n",
    "\n",
    "Para dos variables categóricas \\(X\\) y \\(Y\\), la Información Mutua \\(I(X; Y)\\) se calcula utilizando la siguiente fórmula:\n",
    "\n",
    "$I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log\\left(\\frac{p(x, y)}{p(x)p(y)}\\right)$  \n",
    "donde:\n",
    "- \\(p(x, y)\\) es la probabilidad conjunta de que \\(X = x\\) y \\(Y = y\\).\n",
    "- \\(p(x)\\) y \\(p(y)\\) son las probabilidades marginales de \\(X\\) y \\(Y\\), respectivamente.\n",
    "- $(\\log)$ es el logaritmo natural, aunque en algunas aplicaciones se puede utilizar la base 2 para medir la información en bits.\n",
    "\n",
    "### Pasos para Calcular la Información Mutua\n",
    "\n",
    "1. **Calcular las Probabilidades Marginales y Conjuntas**: Primero, se determinan las probabilidades marginales de cada valor posible de \\(X\\) y \\(Y\\) por separado, \\(p(x)\\) y \\(p(y)\\), así como la probabilidad conjunta \\(p(x, y)\\).\n",
    "\n",
    "2. **Aplicar la Fórmula de Información Mutua**: Se utiliza la fórmula mencionada anteriormente para calcular la suma de las probabilidades conjuntas multiplicadas por el logaritmo de la razón entre la probabilidad conjunta y el producto de las probabilidades marginales.\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "- Un valor de $I(X; Y) = 0$ indica que $X$ e $Y$ son completamente independientes, es decir, el conocimiento de $X$ no proporciona ninguna información sobre $Y$, y viceversa.\n",
    "- Un valor mayor de $I(X; Y)$ indica una mayor dependencia o información compartida entre $X$ y $Y$ual_information.\n",
    "\n",
    "La Información Mutua es útil en muchas aplicaciones de análisis de datos y aprendizaje automático, especialmente para la selección de características y para entender las relaciones entre variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
